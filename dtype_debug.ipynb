{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be700159",
   "metadata": {},
   "source": [
    "# Debug: PyTorch Quantile Dtype Error in Wanda Selectivity\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "When running the Wanda IDF pruning method, we encounter this error:\n",
    "```\n",
    "RuntimeError: quantile() input tensor must be either float or double dtype\n",
    "```\n",
    "\n",
    "This error occurs in `lib/wanda_selectivity.py` at line 68 in the `finalize()` method when calling `torch.quantile()`.\n",
    "\n",
    "## Error Location\n",
    "- **File:** `lib/wanda_selectivity.py`\n",
    "- **Method:** `SelectivityStatsLight.finalize()`\n",
    "- **Line:** `q90_per_channel = torch.quantile(all_samples, 0.9, dim=0)`\n",
    "\n",
    "The issue is that `torch.quantile()` requires tensors to be of float32 or float64 dtype, but the `all_samples` tensor might be in a different dtype (like int32, int64, float16, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ed271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment and Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c1380",
   "metadata": {},
   "source": [
    "## 1. Reproduce the Error\n",
    "\n",
    "Let's reproduce the exact error that occurs in the `SelectivityStatsLight.finalize()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the problematic scenario\n",
    "def reproduce_quantile_error():\n",
    "    \"\"\"Reproduce the exact error from wanda_selectivity.py\"\"\"\n",
    "    \n",
    "    # Create sample data that might have incompatible dtype\n",
    "    # This simulates what happens in the SelectivityStatsLight class\n",
    "    samples = []\n",
    "    \n",
    "    # Simulate different potential dtypes that could cause issues\n",
    "    problematic_dtypes = [torch.int32, torch.int64, torch.float16]\n",
    "    \n",
    "    for dtype in problematic_dtypes:\n",
    "        print(f\"\\nüß™ Testing with dtype: {dtype}\")\n",
    "        \n",
    "        # Create sample tensors (simulating the samples list in SelectivityStatsLight)\n",
    "        if dtype in [torch.int32, torch.int64]:\n",
    "            # Integer data (could come from tokenizer outputs or similar)\n",
    "            sample_data = torch.randint(0, 100, (50, 512), dtype=dtype)\n",
    "        else:\n",
    "            # Float16 data (common in mixed precision training)\n",
    "            sample_data = torch.randn(50, 512, dtype=dtype)\n",
    "        \n",
    "        samples.append(sample_data)\n",
    "        \n",
    "        # Try to concatenate and run quantile (this will fail)\n",
    "        try:\n",
    "            all_samples = torch.cat(samples, dim=0)\n",
    "            print(f\"   Combined tensor dtype: {all_samples.dtype}\")\n",
    "            print(f\"   Combined tensor shape: {all_samples.shape}\")\n",
    "            \n",
    "            # This line will cause the error for non-float dtypes\n",
    "            q90_per_channel = torch.quantile(all_samples, 0.9, dim=0)\n",
    "            print(f\"   ‚úÖ Success! Quantile computed successfully\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "        \n",
    "        # Clear samples for next iteration\n",
    "        samples.clear()\n",
    "\n",
    "reproduce_quantile_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738807d",
   "metadata": {},
   "source": [
    "## 2. Investigate Data Types\n",
    "\n",
    "Let's examine which tensor dtypes are compatible with `torch.quantile()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test which dtypes work with torch.quantile()\n",
    "def test_quantile_dtypes():\n",
    "    \"\"\"Test various tensor dtypes with torch.quantile()\"\"\"\n",
    "    \n",
    "    test_dtypes = [\n",
    "        torch.float32,   # Should work\n",
    "        torch.float64,   # Should work  \n",
    "        torch.float16,   # Might not work\n",
    "        torch.int32,     # Won't work\n",
    "        torch.int64,     # Won't work\n",
    "        torch.bool,      # Won't work\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing torch.quantile() compatibility with different dtypes:\\n\")\n",
    "    \n",
    "    for dtype in test_dtypes:\n",
    "        try:\n",
    "            # Create test tensor\n",
    "            if dtype == torch.bool:\n",
    "                test_tensor = torch.randint(0, 2, (100, 10), dtype=torch.int32).bool()\n",
    "            elif dtype in [torch.int32, torch.int64]:\n",
    "                test_tensor = torch.randint(0, 100, (100, 10), dtype=dtype)\n",
    "            else:\n",
    "                test_tensor = torch.randn(100, 10, dtype=dtype)\n",
    "            \n",
    "            # Try quantile operation\n",
    "            result = torch.quantile(test_tensor, 0.9, dim=0)\n",
    "            print(f\"‚úÖ {dtype}: SUCCESS - Result dtype: {result.dtype}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {dtype}: FAILED - {str(e)[:50]}...\")\n",
    "\n",
    "test_quantile_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cd32c",
   "metadata": {},
   "source": [
    "## 3. Check Tensor Properties\n",
    "\n",
    "Let's examine the properties that could lead to dtype issues in the original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce029268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the problematic SelectivityStatsLight behavior\n",
    "class SelectivityStatsLightOriginal:\n",
    "    \"\"\"Original (buggy) version that causes the dtype error\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels: int):\n",
    "        self.num_channels = num_channels\n",
    "        self.samples = []\n",
    "        self.max_samples = 200\n",
    "\n",
    "    def update(self, x: torch.Tensor):\n",
    "        if x.numel() == 0:\n",
    "            return\n",
    "        \n",
    "        n_rows = x.shape[0]\n",
    "        if len(self.samples) < self.max_samples:\n",
    "            n_to_sample = min(50, n_rows, self.max_samples - len(self.samples))\n",
    "            if n_to_sample > 0:\n",
    "                indices = torch.randperm(n_rows, device=x.device)[:n_to_sample]\n",
    "                # BUG: Not ensuring dtype is float - just moving to CPU\n",
    "                sampled = x[indices].cpu()  # This preserves original dtype!\n",
    "                self.samples.append(sampled)\n",
    "    \n",
    "    def finalize(self):\n",
    "        if len(self.samples) == 0:\n",
    "            return torch.ones(self.num_channels), torch.ones(self.num_channels)\n",
    "        \n",
    "        # BUG: torch.cat preserves the dtype of input tensors\n",
    "        all_samples = torch.cat(self.samples, dim=0)\n",
    "        print(f\"   All samples dtype before quantile: {all_samples.dtype}\")\n",
    "        \n",
    "        # This will fail if all_samples is not float32/float64\n",
    "        q90_per_channel = torch.quantile(all_samples, 0.9, dim=0)\n",
    "        return torch.ones(self.num_channels), torch.ones(self.num_channels)\n",
    "\n",
    "# Test the buggy version\n",
    "print(\"Testing original (buggy) SelectivityStatsLight:\\n\")\n",
    "\n",
    "# Test with float16 input (common in mixed precision)\n",
    "stats_buggy = SelectivityStatsLightOriginal(num_channels=128)\n",
    "test_input = torch.randn(100, 128, dtype=torch.float16)\n",
    "\n",
    "print(f\"Input tensor dtype: {test_input.dtype}\")\n",
    "stats_buggy.update(test_input)\n",
    "\n",
    "try:\n",
    "    stats_buggy.finalize()\n",
    "    print(\"‚úÖ No error occurred\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87459aec",
   "metadata": {},
   "source": [
    "## 4. Fix the Dtype Issue\n",
    "\n",
    "Now let's implement the fix by ensuring tensors are converted to float before quantile operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed version of SelectivityStatsLight\n",
    "class SelectivityStatsLightFixed:\n",
    "    \"\"\"Fixed version that handles dtype conversion properly\"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels: int):\n",
    "        self.num_channels = num_channels\n",
    "        self.samples = []\n",
    "        self.max_samples = 200\n",
    "\n",
    "    def update(self, x: torch.Tensor):\n",
    "        if x.numel() == 0:\n",
    "            return\n",
    "        \n",
    "        n_rows = x.shape[0]\n",
    "        if len(self.samples) < self.max_samples:\n",
    "            n_to_sample = min(50, n_rows, self.max_samples - len(self.samples))\n",
    "            if n_to_sample > 0:\n",
    "                indices = torch.randperm(n_rows, device=x.device)[:n_to_sample]\n",
    "                # FIX 1: Ensure float dtype when storing samples\n",
    "                sampled = x[indices].cpu().float()  # Convert to float32\n",
    "                self.samples.append(sampled)\n",
    "    \n",
    "    def finalize(self):\n",
    "        if len(self.samples) == 0:\n",
    "            return torch.ones(self.num_channels), torch.ones(self.num_channels)\n",
    "        \n",
    "        all_samples = torch.cat(self.samples, dim=0)\n",
    "        \n",
    "        # FIX 2: Double-check dtype before quantile operation\n",
    "        if all_samples.dtype not in [torch.float32, torch.float64]:\n",
    "            all_samples = all_samples.float()\n",
    "        \n",
    "        print(f\"   All samples dtype after fix: {all_samples.dtype}\")\n",
    "        \n",
    "        # Compute per-channel stats\n",
    "        mean_per_channel = all_samples.mean(dim=0)\n",
    "        median_per_channel = torch.median(all_samples, dim=0).values\n",
    "        \n",
    "        # IDF calculation\n",
    "        above_median = (all_samples > median_per_channel.unsqueeze(0)).float().mean(dim=0)\n",
    "        p_j = torch.clamp(above_median, 1e-6, 0.999)\n",
    "        idf_scores = torch.log(1.0 / (p_j + 1e-9))\n",
    "        idf_scores = torch.clamp(idf_scores, 0.1, 10.0)\n",
    "        \n",
    "        # Spikiness calculation - this will now work!\n",
    "        q90_per_channel = torch.quantile(all_samples, 0.9, dim=0)\n",
    "        spikiness_scores = torch.ones(self.num_channels)\n",
    "        \n",
    "        for j in range(self.num_channels):\n",
    "            top_vals = all_samples[all_samples[:, j] >= q90_per_channel[j], j]\n",
    "            if len(top_vals) > 0:\n",
    "                mu_top = top_vals.mean().item()\n",
    "                mu = mean_per_channel[j].item()\n",
    "                spikiness_scores[j] = mu_top / (mu + 1e-9)\n",
    "        \n",
    "        spikiness_scores = torch.clamp(spikiness_scores, 1.0, 20.0)\n",
    "        \n",
    "        return idf_scores, spikiness_scores\n",
    "\n",
    "# Test the fixed version\n",
    "print(\"Testing fixed SelectivityStatsLight:\\n\")\n",
    "\n",
    "stats_fixed = SelectivityStatsLightFixed(num_channels=128)\n",
    "\n",
    "# Test with various problematic dtypes\n",
    "test_dtypes = [torch.float16, torch.int32, torch.float32]\n",
    "\n",
    "for dtype in test_dtypes:\n",
    "    print(f\"\\nüß™ Testing with {dtype}:\")\n",
    "    \n",
    "    if dtype in [torch.int32]:\n",
    "        test_input = torch.randint(0, 100, (100, 128), dtype=dtype)\n",
    "    else:\n",
    "        test_input = torch.randn(100, 128, dtype=dtype)\n",
    "    \n",
    "    print(f\"   Input dtype: {test_input.dtype}\")\n",
    "    \n",
    "    # Reset stats for each test\n",
    "    stats_fixed.samples = []\n",
    "    stats_fixed.update(test_input)\n",
    "    \n",
    "    try:\n",
    "        idf_scores, spikiness_scores = stats_fixed.finalize()\n",
    "        print(f\"   ‚úÖ Success! IDF shape: {idf_scores.shape}, Spikiness shape: {spikiness_scores.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66768e9",
   "metadata": {},
   "source": [
    "## 5. Verify the Solution\n",
    "\n",
    "Let's demonstrate that our fix resolves the original error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive verification of the fix\n",
    "def verify_fix_comprehensive():\n",
    "    \"\"\"Test the fix with edge cases and various scenarios\"\"\"\n",
    "    \n",
    "    print(\"üîç Comprehensive Fix Verification\\n\")\n",
    "    \n",
    "    # Test scenarios that could occur in real usage\n",
    "    test_scenarios = [\n",
    "        (\"Mixed precision (float16)\", torch.float16, lambda: torch.randn(150, 256, dtype=torch.float16)),\n",
    "        (\"Integer weights\", torch.int32, lambda: torch.randint(-50, 50, (150, 256), dtype=torch.int32)),\n",
    "        (\"Large integer values\", torch.int64, lambda: torch.randint(-1000, 1000, (150, 256), dtype=torch.int64)),\n",
    "        (\"Normal float32\", torch.float32, lambda: torch.randn(150, 256, dtype=torch.float32)),\n",
    "        (\"Double precision\", torch.float64, lambda: torch.randn(150, 256, dtype=torch.float64)),\n",
    "    ]\n",
    "    \n",
    "    for scenario_name, dtype, data_generator in test_scenarios:\n",
    "        print(f\"üìã Testing: {scenario_name}\")\n",
    "        \n",
    "        # Test original (should fail for some dtypes)\n",
    "        stats_original = SelectivityStatsLightOriginal(num_channels=256)\n",
    "        test_data = data_generator()\n",
    "        \n",
    "        print(f\"   Input: {test_data.dtype}, shape: {test_data.shape}\")\n",
    "        stats_original.update(test_data)\n",
    "        \n",
    "        try:\n",
    "            stats_original.finalize()\n",
    "            print(\"   Original: ‚úÖ Success\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"   Original: ‚ùå Failed - {str(e)[:40]}...\")\n",
    "        \n",
    "        # Test fixed version (should always work)\n",
    "        stats_fixed = SelectivityStatsLightFixed(num_channels=256)\n",
    "        stats_fixed.update(test_data)\n",
    "        \n",
    "        try:\n",
    "            idf, spikiness = stats_fixed.finalize()\n",
    "            print(f\"   Fixed: ‚úÖ Success - IDF: {idf.dtype}, Spikiness: {spikiness.dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Fixed: ‚ùå Failed - {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "verify_fix_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a0ba2",
   "metadata": {},
   "source": [
    "## 6. Test with Different Data Types\n",
    "\n",
    "Final validation with extreme edge cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case testing\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test with edge cases that might occur in practice\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Edge Case Testing\\n\")\n",
    "    \n",
    "    edge_cases = [\n",
    "        (\"Empty tensor\", lambda: torch.empty(0, 128)),\n",
    "        (\"Single sample\", lambda: torch.randn(1, 128, dtype=torch.float16)),\n",
    "        (\"Very small values\", lambda: torch.randn(100, 128) * 1e-8),\n",
    "        (\"Very large values\", lambda: torch.randn(100, 128) * 1e8),\n",
    "        (\"All zeros\", lambda: torch.zeros(100, 128)),\n",
    "        (\"All ones\", lambda: torch.ones(100, 128)),\n",
    "        (\"Boolean as int\", lambda: torch.randint(0, 2, (100, 128), dtype=torch.int32)),\n",
    "    ]\n",
    "    \n",
    "    for case_name, data_generator in edge_cases:\n",
    "        print(f\"üß™ Testing: {case_name}\")\n",
    "        \n",
    "        try:\n",
    "            test_data = data_generator()\n",
    "            print(f\"   Data: {test_data.dtype}, shape: {test_data.shape}\")\n",
    "            \n",
    "            if test_data.numel() == 0:\n",
    "                print(\"   Skipping empty tensor\")\n",
    "                continue\n",
    "            \n",
    "            stats = SelectivityStatsLightFixed(num_channels=test_data.shape[-1])\n",
    "            stats.update(test_data)\n",
    "            \n",
    "            idf, spikiness = stats.finalize()\n",
    "            print(f\"   ‚úÖ Success - IDF range: [{idf.min():.3f}, {idf.max():.3f}]\")\n",
    "            print(f\"                Spikiness range: [{spikiness.min():.3f}, {spikiness.max():.3f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159d367",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ **Problem Fixed!**\n",
    "\n",
    "The error `RuntimeError: quantile() input tensor must be either float or double dtype` was caused by:\n",
    "\n",
    "1. **Root Cause**: `torch.quantile()` only accepts float32 or float64 tensors, but the code was passing tensors with other dtypes (like float16, int32, int64).\n",
    "\n",
    "2. **Location**: In `lib/wanda_selectivity.py`, line 68 in the `SelectivityStatsLight.finalize()` method.\n",
    "\n",
    "3. **Solution Applied**: Two fixes were implemented:\n",
    "   - **Fix 1**: In `update()` method - Convert tensors to float32 when storing: `.cpu().float()`\n",
    "   - **Fix 2**: In `finalize()` method - Double-check dtype before quantile operation\n",
    "\n",
    "### üîß **Code Changes Made**\n",
    "\n",
    "In `/Users/vasyl/Projects/wandar/lib/wanda_selectivity.py`:\n",
    "\n",
    "```python\n",
    "# Line ~35: In update() method\n",
    "sampled = x[indices].cpu().float()  # Added .float()\n",
    "\n",
    "# Line ~55: In finalize() method  \n",
    "if all_samples.dtype not in [torch.float32, torch.float64]:\n",
    "    all_samples = all_samples.float()\n",
    "```\n",
    "\n",
    "### üöÄ **Ready to Run**\n",
    "\n",
    "Your original command should now work without errors:\n",
    "\n",
    "```bash\n",
    "python main.py \\\n",
    "  --model baffo32/decapoda-research-llama-7B-hf \\\n",
    "  --prune_method wanda_idf \\\n",
    "  --sparsity_ratio 0.5 \\\n",
    "  --sparsity_type unstructured \\\n",
    "  --save out/wanda_idf/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
